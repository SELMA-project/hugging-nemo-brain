name: &name "debug"
sample_rate: &sample_rate 16000
repeat: &rep 2 # TODO(tilo): defining model architecture ? or what else could be repeated?
dropout: &drop 0.5
separable: &separable True
n_filters: &n_filters 512
pretrained_model: "speakerrecognition_speakernet"
freeze_encoder: True
subset_labels: ["en","de","fr","es","ru","zh-CN","OTHER"]

model:
  train_ds:
    manifest_filepath: "${BASE_PATH}/data/lang_clf_data/train_manifest.jsonl"
    sample_rate: 16000
    labels: null
    batch_size: 128
    num_workers: 12
    shuffle: True
    time_length: 8
    shift_lenghts: null # tilo: this is NOT used!
    is_tarred: False
    tarred_audio_filepaths: null
    tarred_shard_strategy: "scatter"

  validation_ds:
    manifest_filepath: "${BASE_PATH}/data/lang_clf_data/validation_manifest.jsonl"
    sample_rate: 16000
    labels: null
    batch_size: 128
    num_workers: 8
    shuffle: False
    time_length: 8

  test_ds:
    manifest_filepath: null
    sample_rate: 16000
    labels: null
    batch_size: 1
    shuffle: False
    time_length: 8
    embedding_dir: '.'

  preprocessor:
    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor
    normalize: "per_feature"
    window_size: 0.02
    sample_rate: *sample_rate
    window_stride: 0.01
    window: "hann"
    features: &n_mels 64
    n_fft: 512
    frame_splicing: 1
    dither: 0.00001

  encoder:
    _target_: nemo.collections.asr.modules.ConvASREncoder
    feat_in: *n_mels
    activation: relu
    conv_mask: true

    jasper:
      -   filters: *n_filters
          repeat: 1
          kernel: [3]
          stride: [1]
          dilation: [1]
          dropout: *drop
          residual: true
          separable: *separable

      -   filters: *n_filters
          repeat: *rep
          kernel: [7]
          stride: [1]
          dilation: [1]
          dropout: *drop
          residual: true
          separable: *separable

      -   filters: *n_filters
          repeat: *rep
          kernel: [11]
          stride: [1]
          dilation: [1]
          dropout: *drop
          residual: true
          separable: *separable

      -   filters: *n_filters
          repeat: *rep
          kernel: [15]
          stride: [1]
          dilation: [1]
          dropout: *drop
          residual: true
          separable: *separable

      -   filters: &enc_feat_out 1500
          repeat: 1
          kernel: [1]
          stride: [1]
          dilation: [1]
          dropout: 0.0
          residual: false
          separable: *separable

  decoder:
    _target_: nemo.collections.asr.modules.SpeakerDecoder
    feat_in: *enc_feat_out
    num_classes: null # tilo: changed from 7205
    pool_mode: 'xvector'
    emb_sizes: 512,512
    angular: False

  loss:
    scale: 30
    margin: 0.2

  optim:
    name: novograd
    # _target_: nemo.core.optim.optimizers.Novograd
    lr: .008
    # optimizer arguments
    args:
      name: auto
      # _target_: nemo.core.config.optimizers.NovogradParams
      betas: [0.95, 0.5]
      weight_decay: 0.001

    # scheduler setup
    sched:
#      name: CosineAnnealing
#      iters_per_batch: 1 # computed at runtime
#      max_steps: null # computed at runtime or explicitly set here
#
#      # scheduler config override
#      args:
#        name: auto
#        # _target_: nemo.core.config.schedulers.CosineAnnealingParams
#        warmup_steps: null
#        warmup_ratio: 0.1
#        min_lr: 0.0
#        last_epoch: -1
    # tilo: I prefere simple scheduler, constant until plateau sounds good to me
      name: ReduceLROnPlateau
      mode: min
      factor: 0.8
      patience: 5
      verbose: True
      threshold: 0.01
      threshold_mode: 'rel'
      cooldown: 0
      min_lr: 5.e-5
      eps: 1.e-08

trainer:
  gpus: 1 # number of gpus
  max_epochs: 9
  max_steps: null # computed at runtime if not set
  num_nodes: 1
  accelerator: ddp
  accumulate_grad_batches: 1
  amp_level: O0
  deterministic: True
  checkpoint_callback: False
  logger: False
  log_every_n_steps: 10  # Interval of logging.
  val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations

hydra:
  run:
    dir: .
  job_logging:
    root:
      handlers: null

exp_manager:
  exp_dir: ${BASE_PATH}/results/TRAINING/LANG_CLF
  name: *name
  create_wandb_logger: True
  wandb_logger_kwargs:
    name: *name
    project: "nemo-lang-clf"

  create_tensorboard_logger: False
  create_checkpoint_callback: True
  #  checkpoint_callback_params:
  #    monitor: "val_wer"
  #    mode: "min"
  #    save_top_k: 3
  #
